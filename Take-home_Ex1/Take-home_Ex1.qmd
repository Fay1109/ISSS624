---
title: "Take-home Exercise 1"
execute:
   warning: false
   message: false
editor: visual
---

## Overview

Water is an important resource to human beings. Providing clean and accessible water is crucial for human health. However, over 40% of population worldwide do not have access to clean water. The lack of clean water not only poses a threat to human health, but also threatens food security as 70% of world's clean water is used in agriculture. This problem is especially severe in Africa continent due to the difficulty in providing clean water to the rural community.

To solve the above issue, a global [Water Point Data Exchange (WPdx)](https://www.waterpointdata.org/about/) project has been initiated which aims at collecting water point related data from rural areas and share it via WPdx Data Repository.

## Objectives

Geospatial analytics plays an important role in addressing social issues. In this study, appropriate global and local measures of spatial association techniques are used to identify the spatial patterns of Not Functional water points. Nigeria is used as the study country

## Introduction

## Libraries

-   sf is use for importing and handling geospatial data in R,

-   tidyverse is mainly use for wrangling attribute data in R,

-   spdep will be used to compute spatial weights, global and local spatial autocorrelation statistics, and

-   tmap will be used to prepare cartographic quality chropleth map.

## Getting Started

```{r}
pacman::p_load(sf, tidyverse, tmap, spdep)
```

## Importing Geospatial Data

read_rds is used to access the data.

```{r}
nga_wp <- read_rds("data/nga_wp.rds")
```

```{r}
nigeria <- nga_wp
```

```{r}
wp_functional <- qtm(nga_wp, "wpt functional")
wp_nonfunctional <- qtm(nga_wp, "wpt non-functional")
```

## Importing water point geospatial data

First, we are going to import the water point geospatial data (i.e. geo_export) by using the code chunk below.

```{r}
#| eval: false
wp <- st_read(dsn = "data", 
                 layer = "geo_export",
                  crs = 4326) %>%
  filter(clean_coun == "Nigeria")
```

Next, `write_rds()` of readr package is used to save the extracted sf data table (i.e. wp) into an output file in rds data format. The output file is called *wp_nga.rds* and it is saved in *geodata* sub-folder.

```{r}
#| eval: false
write_rds(wp, "data/wp_nga.rds")
```

## Importing Nigeria LGA boundary data

Now, we are going to import the LGA boundary data into R environment by using the code chunk below.

```{r}
#| eval: false
nga <- st_read(dsn = "data",layer = "nga_admbnda_adm2_osgof_20190417",crs = 4326)
```

## Data Wrangling

## Recoding NA values into string

In the code chunk below, `replace_na()` is used to recode all the *NA* values in *status_cle* field into *Unknown*.

```{r}
#| eval: false
wp_nga <- write_rds(wp, "data/wp_nga.rds")
```

```{r}
#| eval: false
wp_nga <- read_rds("data/wp_nga.rds") %>%
  mutate(status_cle = 
replace_na(status_cle, "Unknown"))
```

## EDA

In the code chunk below. 'filter()' of dplyr is used to select functional points.

```{r}
#| eval: false
freq(data=wp_nga,
     input = 'status_cle')
```

## Extracting Water Point Data

In this section, we will extract the water point records by using classes in *status_cle* field.

## Extracting funtional water point

In the code chunk below, `filter()` of dplyr is used to select functional water points.

```{r}
#| eval: false
wpt_functional <- wp_nga %>%
  filter(status_cle %in%
           c("Functional",
             "Functional but not in use",
             "Functional but needs repair"))
```

```{r}
#| eval: false
freq(data=wpt_functional,
     input = 'status_cle')
```

## Extracting non-funtional water point

In the code chunk below, `filter()` of dplyr is used to select non-functional water points.

```{r}
#| eval: false
wpt_nonfunctional <- wp_nga %>%
  filter(status_cle %in%
           c("Abandoned/Decommissioned",
             "Abandoned",
             "Non-Functional",
             "Non functional due to dry season",
             "Non functional due to dry season"))
```

```{r}
#| eval: false
freq(data=wpt_nonfunctional,
     input = 'status_cle')
```

## Extracting water point with Unknown class

In the code chunk below, `filter()` of dplyr is used to select water points with unknown status.

```{r}
#| eval: false
wpt_unknown <- wp_nga %>%
  filter(status_cle == "Unknown")
```

## Performing Point-in-Polygon Count

```{r}
#| eval: false
nga_wp <- nga %>% 
  mutate(`total wpt` = lengths(
    st_intersects(nga, wp_nga))) %>%
  mutate(`wpt functional` = lengths(
    st_intersects(nga, wpt_functional))) %>%
  mutate(`wpt non-functional` = lengths(
    st_intersects(nga, wpt_nonfunctional))) %>%
  mutate(`wpt unknown` = lengths(
    st_intersects(nga, wpt_unknown)))
```

## Saving the Analytical Data Table

```{r}
#| eval: false
nga_wp <- nga_wp %>%
  mutate(pct_functional = `wpt functional`/`total wpt`) %>%
  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`) %>%
  select(3:4, 9:10, 18:23)
```

Things to learn from the code chunk above:

-   `mutate()` of **dplyr** package is used to derive two fields namely *pct_functional* and *pct_non-functional*.

-   to keep the file size small, `select()` of **dplyr** is used to retain only field 3,4,9,10, 18,19,20,21,22,and 23.

Now, you have the tidy sf data table subsequent analysis. We will save the sf data table into rds format.

```{r}
#| eval: false
write_rds(nga_wp, "data/nga_wp.rds")
```

## Visualising the spatial distribution of water point

```{r}
#| fig-width: 14
#| fig-height: 12
nga_wp <- read_rds("data/nga_wp.rds")
total <- qtm(nga_wp, "total wpt")
wp_functional <- qtm(nga_wp, "wpt functional")
wp_nonfunctional <- qtm(nga_wp, "wpt non-functional")
unknown <- qtm(nga_wp, "wpt unknown")

tmap_arrange(total, wp_functional, wp_nonfunctional, unknown, asp=1, ncol=2)
```

## Assigning EPSG code to a simple feature data frame

```{r}
st_crs(nga_wp)
```

Projected Coordinate System should be changed to one of following EPSG: 26391, 26392, and 26303. The code chunk below is used to change EPSG to 26391.

```{r}
nga_wp26391 <- st_set_crs(nga_wp, 26391)
```

```{r}
nga_wp<- st_transform(nga_wp, 26391)
st_crs(nga_wp)
st_crs(nga_wp26391)
```

## Functional and non-functional water point

## Thematic Mapping

By using appropriate thematic mapping technique provided by tmap package, spatial distribution of functional and non-functional water point rate are demonstrated at LGA level.

```{r}
equal <- tm_shape(nga_wp) +
  tm_fill("wpt non-functional",
          n = 6,
          style = "equal") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal interval")

quantile <- tm_shape(nga_wp) +
  tm_fill("wpt non-functional",
          n = 6,
          style = "quantile") +
  tm_borders(alpha = 0.5) +
  tm_layout(main.title = "Equal quantile")

tmap_arrange(equal, 
             quantile, 
             asp=1, 
             ncol=2)
```

## Computing Contiguity Spatial Weights

Is the distribution of functional water points across Nigeria equal?

Before we compute the global spatial autocorrelation statistics, we need to construct a spatial weights. The spatial weights is used to define the neighbourhood relationships between the local government areas in Nigeria.

First of all, we need to select an appropriate spatial weighing method to calculate the spatial weight matrix.

### Polygon Contiguity Method

The **polygon contiguity method** is effective when polygons are similar in size and distribution, and when spatial relationships are a function of polygon proximity (the idea that if two polygons share a boundary, spatial interaction between them increases).

The code chunk below is used to show the boundary line of LGA in Nigeria.

```{r}
nigeria_lga <- tm_shape(nga_wp) +
  tm_polygons()
nigeria_lga
```

The size and distribution of LGA in Nigeria is not similar. Some LGA are more dense and have shorter boundary.

### Distance-based neighbours

To use distance-based method, the first step is to get the centroid of each polygon by running *st_centroid* on the **sf** package.

```{r}
coords <- st_centroid(st_geometry(nga_wp))
```

```{r}
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first-nearest neighbour distance is 72139 km, which will be used as the upper threshold in order to make sure all LGA will have at least one neighbour.

### Computing fixed distance weight matrix

The **fixed distance method** often is a good option for polygon data when there is a large variation in polygon size.

```{r}
upper_therhold <- 72200
wm_d <- dnearneigh(coords, 0, upper_therhold, longlat = TRUE)
wm_d
```

The report shows that the average number of links for each region is 23.46, which may be skewed for the analysis.

### Computing adaptive distance weight matrix

Adaptive distance can adjust itself according to the density of data. K-nearest neighbours can be used to control the numbers of neighbours directly. The numbers of neighbours can be assigned to *knearhneigh()* and the neighbours can be designated based on the distance between centroids.

The code chunk below is used to control the numbers of neighbours at 8.

```{r}
knn8 <- knn2nb(knearneigh(coords, k=8))
knn8
```

The code chunk below is used to plot the adaptive distance based neighbours.

```{r}
plot(nga_wp$geometry, border="lightgrey")
plot(knn8, coords, pch = 19, cex = 0.6, add = TRUE, col = "red")
```

**K-nearest neighbours method** is chosen as the spatial weighting method.

### Row-standardised weight matrix

Next, we need to assign weights to each neighbouring polygon. In our case, each neighbouring will be assigned equal weight.

```{r}
rsknn8 <- nb2listw(knn8, 
                   style="W", 
                   zero.policy = TRUE)
rsknn8
```

## Global Spatial Autocorrelation

### Global Spatial Autocorrelation: Moran's I

#### Moran's I Test

The code chunk below performs Moran's I statistical testing using *moran.test()* of **spdep**.

```{r}
moran.test(nga_wp$`pct_non-functional`, 
           listw=rsknn8, 
           zero.policy = TRUE, 
           na.action=na.omit)
```

#### Computing Monte Carlo Moran's I

The code chunk below performs permutation test for Moran's I statistic by using *moran.mc()* of **spdep**. A total of 1000 simulation will be performed.

```{r}
set.seed(1234)
bperm= moran.mc(nga_wp$`pct_non-functional`, 
                listw=rsknn8, 
                nsim=999, 
                zero.policy = TRUE, 
                na.action=na.omit)
bperm
```

#### Visualising Monte Carlo Moran's I

The code chunk below is used to plot the distribution of Monte Carlo test result.

```{r}
mean(bperm$res[1:999])
var(bperm$res[1:999])
summary(bperm$res[1:999])
```

```{r}
hist(bperm$res, 
     freq=TRUE, 
     breaks=20, 
     xlab="Simulated Moran's I")
abline(v=0, 
       col="red") 
```

#### Computing Moran's I correlogram

In the code chunk below, *sp.correlogram()* of spdep package is used to compute a 8-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran's I. The plot() of base Graph is then used to plot the output.

```{r}
MI_corr <- sp.correlogram(knn8, 
                          nga_wp$`wpt non-functional`, 
                          order=8, 
                          method="I", 
                          style="W")
plot(MI_corr)
```

## Cluster and Outlier Analysis

Local Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable.

## Computing local Moran's I

To compute local Moran's I, the [*localmoran()*](https://r-spatial.github.io/spdep/reference/localmoran.html) function of **spdep** will be used. It computes *Ii* values, given a set of *zi* values and a listw object providing neighbour weighting information for the polygon associated with the zi values.

```{r}
fips <- order(nga_wp$`wpt non-functional`)
localMI <- localmoran(nga_wp$`wpt non-functional`, rsknn8)
head(localMI)
```

## Mapping the local Moran's

Using choropleth mapping functions of **tmap** package, we can plot the local Moran's I values by using the code chinks below.

```{r}
nga_wp.localMI <- cbind(nga_wp,localMI) %>%
  rename(Pr.Ii = Pr.z....E.Ii..)
```

```{r}
tm_shape(nga_wp.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty",
          palette = "RdBu",
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)
```

## Mapping both local Moran's I values and p-values

For effective interpretation, it is better to plot both the local Moran's I values map and its corresponding p-values map next to each other.

The code chunk below will be used to create such visualisation.

```{r}
localMI.map <- tm_shape(nga_wp.localMI) +
  tm_fill(col = "Ii", 
          style = "pretty", 
          title = "local moran statistics") +
  tm_borders(alpha = 0.5)

pvalue.map <- tm_shape(nga_wp.localMI) +
  tm_fill(col = "Pr.Ii", 
          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),
          palette="-Blues", 
          title = "local Moran's I p-values") +
  tm_borders(alpha = 0.5)

tmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)
```

## Creating a LISA Cluster Map

The LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.

### Plotting Moran scatterplot

The Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.

```{r}
nci <- moran.plot(nga_wp$`wpt non-functional`, rsknn8,
                  labels=as.character(nga_wp$ADM2_EN), 
                  xlab="Non-Functional waterpoints", 
                  ylab="Spatially Lag Non-functional waterpoints")
```

### Plotting Moran scatterplot with standardised variable

```{r}
nga_wp$Z.nonfunc <- scale(nga_wp$`wpt non-functional`) %>% 
  as.vector 
nci2 <- moran.plot(nga_wp$Z.nonfunc, rsknn8,
                   labels=as.character(nga_wp$ADM2_EN),
                   xlab="z-no functional points", 
                   ylab="Spatially Lag non functional points")
```

## Preparing LISA map classes

The code chunks below show the steps to prepare a LISA cluster map.

```{r}
quadrant <- vector(mode="numeric",length=nrow(localMI))
```

Next, derives the spatially lagged variable of interest and centers the spatially lagged variable around its mean.

```{r}
nga_wp$lag_non_func_points <- lag.listw(rsknn8, nga_wp$`wpt non-functional`)
DV <- nga_wp$lag_non_func_points - mean(nga_wp$lag_non_func_points)
```

```{r}
LM_I <- localMI[,1] - mean(localMI[,1])    
```

```{r}
signif <- 0.05       
```

```{r}
quadrant[DV <0 & LM_I>0] <- 1
quadrant[DV >0 & LM_I<0] <- 2
quadrant[DV <0 & LM_I<0] <- 3  
quadrant[DV >0 & LM_I>0] <- 4      
```

```{r}
quadrant[localMI[,5]>signif] <- 0
```

## Plotting LISA map

Now, we can build the LISA map by using the code chunks below.

```{r}
nga_wp.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

tm_shape(nga_wp.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)
```

For effective interpretation, it is better to plot both the local Moran's I values map and its corresponding p-values map next to each other.

The code chunk below will be used to create such visualisation.

```{r}
gdppc <- qtm(nga_wp,"wpt non-functional")

nga_wp.localMI$quadrant <- quadrant
colors <- c("#ffffff", "#2c7bb6", "#abd9e9", "#fdae61", "#d7191c")
clusters <- c("insignificant", "low-low", "low-high", "high-low", "high-high")

LISAmap <- tm_shape(nga_wp.localMI) +
  tm_fill(col = "quadrant", 
          style = "cat", 
          palette = colors[c(sort(unique(quadrant)))+1], 
          labels = clusters[c(sort(unique(quadrant)))+1],
          popup.vars = c("")) +
  tm_view(set.zoom.limits = c(11,17)) +
  tm_borders(alpha=0.5)

tmap_arrange(wp_nonfunctional, LISAmap, 
             asp=1, ncol=2)
```

## Hot Spot and Cold Spot Area Analysis

Beside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.

## Getis and Ord's G-Statistics

The analysis consists of three steps:

-   Deriving spatial weight matrix

-   Computing Gi statistics

-   Mapping Gi statistics

## Deriving distance-based weight matrix

## Deriving the centroid

```{r}
longitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]])
latitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]])
coords <- cbind(longitude, latitude)
```

## Determine the cut-off distance

Firstly, we need to determine the upper limit for distance band by using the steps below:

-   Return a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using [*knearneigh()*](https://r-spatial.github.io/spdep/reference/knearneigh.html) of **spdep**.

-   Convert the knn object returned by *knearneigh()* into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using [*knn2nb()*](https://r-spatial.github.io/spdep/reference/knn2nb.html).

-   Return the length of neighbour relationship edges by using [*nbdists()*](https://r-spatial.github.io/spdep/reference/nbdists.html) of **spdep**. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.

-   Remove the list structure of the returned object by using [**unlist()**](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/unlist).

```{r}
coords <- st_centroid(st_geometry(nga_wp))
```

```{r}
#coords <- coordinates(nga_wp)
k1 <- knn2nb(knearneigh(coords))
k1dists <- unlist(nbdists(k1, coords, longlat = TRUE))
summary(k1dists)
```

The summary report shows that the largest first nearest neighbour distance is 72.139 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.

## Computing fixed distance weight matrix

Now, we will compute the distance weight matrix by using [*dnearneigh()*](https://r-spatial.github.io/spdep/reference/dnearneigh.html) as shown in the code chunk below.

```{r}
wm_d72 <- dnearneigh(coords, 0, 720000, longlat = TRUE)
wm_d72
```

```{r}
wm72_lw <- nb2listw(wm_d72, style = 'B')
summary(wm72_lw)
```

## Computing adaptive distance weight matrix

```{r}
knn <- knn2nb(knearneigh(coords, k=8))
knn
```

```{r}
knn_lw <- nb2listw(knn, style = 'B')
summary(knn_lw)
```

## Computing Gi statistics

## Gi statistics using fixed distance

```{r}
fips <- order(nga_wp$`wpt non-functional`)
gi.fixed <- localG(nga_wp$`wpt non-functional`, wm72_lw)
gi.fixed
nga_wp.gi <- cbind(nga_wp, as.matrix(gi.fixed)) %>%
  rename(gstat_fixed = as.matrix.gi.fixed.)
```

## 

Mapping fixed distance with weights

The code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.

```{r}
nonfunc <- qtm(nga_wp, "wpt non-functional")

Gimap <-tm_shape(nga_wp.gi) +
  tm_fill(col = "gstat_fixed", 
          style = "pretty",
          palette="-RdBu",
          title = "local Gi") +
  tm_borders(alpha = 0.5)

tmap_arrange(gdppc, Gimap, asp=1, ncol=2)
```

## Gi statistics using adaptive distance

```{r}
fips <- order(nga_wp$`wpt non-functional`)
gi.adaptive <- localG(nga_wp$'wpt non-functional', knn_lw)
hunan.gi <- cbind(nga_wp, as.matrix(gi.adaptive)) %>%
  rename(gstat_adaptive = as.matrix.gi.adaptive.)
```

## Mapping Gi values with adaptive distance weights

```{r}
nonfunc <- qtm(nga_wp, "wpt non-functional")

Gimap <- tm_shape(hunan.gi) + 
  tm_fill(col = "gstat_adaptive", 
          style = "pretty", 
          palette="-RdBu", 
          title = "local Gi") + 
  tm_borders(alpha = 0.5)

tmap_arrange(nonfunc, 
             Gimap, 
             asp=1, 
             ncol=2)
```
